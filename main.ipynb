{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46d4ce8d1c3e51a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed9d61a2e2d4efe6"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-04T05:59:13.891941Z",
     "start_time": "2024-02-04T05:59:11.658253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/urllib3/util/url.py\", line 425, in parse_url\r\n",
      "    host, port = _HOST_PORT_RE.match(host_port).groups()  # type: ignore[union-attr]\r\n",
      "AttributeError: 'NoneType' object has no attribute 'groups'\r\n",
      "\r\n",
      "The above exception was the direct cause of the following exception:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/adapters.py\", line 454, in send\r\n",
      "    conn = self.get_connection(request.url, proxies)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/adapters.py\", line 343, in get_connection\r\n",
      "    proxy = prepend_scheme_if_needed(proxy, \"http\")\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/utils.py\", line 993, in prepend_scheme_if_needed\r\n",
      "    parsed = parse_url(url)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/urllib3/util/url.py\", line 451, in parse_url\r\n",
      "    raise LocationParseError(source_url) from e\r\n",
      "urllib3.exceptions.LocationParseError: Failed to parse: http://localhost:File\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/PT46FV/.pyenv/versions/3.9.18/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n",
      "    return _run_code(code, main_globals, None,\r\n",
      "  File \"/Users/PT46FV/.pyenv/versions/3.9.18/lib/python3.9/runpy.py\", line 87, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/spacy/__main__.py\", line 4, in <module>\r\n",
      "    setup_cli()\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/spacy/cli/_util.py\", line 87, in setup_cli\r\n",
      "    command(prog_name=COMMAND)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\r\n",
      "    return self.main(*args, **kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\n",
      "    return _main(\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\r\n",
      "    rv = self.invoke(ctx)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\r\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\r\n",
      "    return ctx.invoke(self.callback, **ctx.params)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\r\n",
      "    return __callback(*args, **kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\n",
      "    return callback(**use_params)  # type: ignore\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/spacy/cli/download.py\", line 36, in download_cli\r\n",
      "    download(model, direct, sdist, *ctx.args)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/spacy/cli/download.py\", line 70, in download\r\n",
      "    compatibility = get_compatibility()\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/spacy/cli/download.py\", line 94, in get_compatibility\r\n",
      "    r = requests.get(about.__compatibility__)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/api.py\", line 73, in get\r\n",
      "    return request(\"get\", url, params=params, **kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/api.py\", line 59, in request\r\n",
      "    return session.request(method=method, url=url, **kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\r\n",
      "    resp = self.send(prep, **send_kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\r\n",
      "    r = adapter.send(request, **kwargs)\r\n",
      "  File \"/Users/PT46FV/Developer/Learning/master-nlp/fnlp/venv/lib/python3.9/site-packages/requests/adapters.py\", line 456, in send\r\n",
      "    raise InvalidURL(e, request=request)\r\n",
      "requests.exceptions.InvalidURL: Failed to parse: http://localhost:File\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install pypdf spacy pandas wordcloud\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg\n",
    "import re\n",
    "from pathlib import Path\n",
    "# import PyPDF2 as pdf2\n",
    "import pypdf as pdf2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download(['stopwords', 'wordnet'])\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configs\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0f7fcbe03deb102"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ignore_pages = {\n",
    "    \"14 REMARKS ON COLOUR\": [0, 35],\n",
    "    \"13 ON CERTAINTY\": [0, 1, 51],\n",
    "    \"12 LAST WRITINGS ON THE PHILOSOPHY OF PSYCHOLOGY 2\": [0, 1, 2, 3, 44, 45],\n",
    "    \"11 LAST WRITINGS ON THE PHILOSOPHY OF PSYCHOLOGY\": [0, 1, 73, 74, 75, 76],\n",
    "    \"10 REMARKS ON THE PHILOSOPHY OF PSYCHOLOGY 2\": [0, 1, 68, 69, 70, 71, 72, 73, 74, 75],\n",
    "    \"09 REMARKS ON THE PHILOSOPHY OF PSYCHOLOGY 1\": [0, 1, 2, 110, 111],\n",
    "    \"08 PHILOSOPHICAL INVESTIGATIONS\": [0, 1, 2, 3, 4],\n",
    "    \"07 ZETTEL\": [0, 1, 2],\n",
    "    \"06 Culture and Value\": [0, 1, 2, 3, 4, 5, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107],\n",
    "    \"05 REMARKS ON THE FOUNDATIONS OF MATHEMATICS\": [0, 1, 2, 3, 4, 5, 172, 173, 174],\n",
    "    \"04 THE BLUE AND THE BROWN BOOKS\": [0, 1, 2, 3, 4, 5, 6, 7, 106],\n",
    "    \"03 PHILOSOPHICAL GRAMMAR\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 227, 228, 229, 230, 231, 232, 233],\n",
    "    \"02 PHILOSOPHICAL REMARKS\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 163, 164, 165, 166, 167, 168, 168, 170, 171],\n",
    "    \"01 Notebooks\": [0, 1, 93, 94, 95, 96, 97, 98, 99],\n",
    "    \"(Routledge Classics) Ludwig Wittgenstein - Tractatus Logico-Philosophicus-Routledge (2001)\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134],\n",
    "    \"Wittgenstein - Notebooks 1914-1916 ed 1961\": []\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T20:10:37.733835Z",
     "start_time": "2024-02-03T20:10:37.719404Z"
    }
   },
   "id": "b6d1a2669140e19e",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Preprocessing\n",
    "## Extract PDFs to txt file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46590b8ec4a1ceb6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _clean_text(text):\n",
    "    text = re.sub(r\"Page( Break)? \\d{1,3}\" , \" \", text)\n",
    "    text = re.sub(r\"Page( Break)? [ivm]{1,3}\" , \" \", text) \n",
    "    # remove capital letter words\n",
    "    text = re.sub(r\"/(\\w*[A-Z]+\\w*)\\w+/\", \"\", text)\n",
    "    text = re.sub(\"\\d{1,5}.\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\" , \" \", text)\n",
    "    text = re.sub(r\"â€ \", \"\", text)\n",
    "    # ( Eds.\\)\n",
    "    text = re.sub(r\"\\( Eds.\\)\", \"\", text)\n",
    "    # [Cf. Z ]\n",
    "    text = re.sub(r\"\\[a-zA-Z\\w*\\]\", \"\", text)\n",
    "    # text = re.sub(r\"\\s+\" , \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_pdf(pdf_file_path, name):\n",
    "    with open (pdf_file_path, \"rb\") as f:\n",
    "        pdf = pdf2.PdfReader(f)\n",
    "        full_text = \"\"\n",
    "        for index, page in enumerate(pdf.pages):\n",
    "            # skip first page and ignored pages\n",
    "            if index not in [0] and index not in ignore_pages[name]:\n",
    "                full_text += _clean_text(page.extract_text() + '\\n')\n",
    "        return full_text\n",
    "    \n",
    "def save_file(data: str, file_name: str):\n",
    "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "    file = Path(\"data\") / file_name\n",
    "    file.write_text(data)\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T20:10:38.727540Z",
     "start_time": "2024-02-03T20:10:38.719649Z"
    }
   },
   "id": "821b3fdf56a1b9d9",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 ZETTEL\n",
      "14 REMARKS ON COLOUR\n",
      "09 REMARKS ON THE PHILOSOPHY OF PSYCHOLOGY 1\n",
      "04 THE BLUE AND THE BROWN BOOKS\n",
      "02 PHILOSOPHICAL REMARKS\n",
      "(Routledge Classics) Ludwig Wittgenstein - Tractatus Logico-Philosophicus-Routledge (2001)\n",
      "10 REMARKS ON THE PHILOSOPHY OF PSYCHOLOGY 2\n",
      "05 REMARKS ON THE FOUNDATIONS OF MATHEMATICS\n",
      "08 PHILOSOPHICAL INVESTIGATIONS\n",
      "01 Notebooks\n",
      "06 Culture and Value\n",
      "11 LAST WRITINGS ON THE PHILOSOPHY OF PSYCHOLOGY\n",
      "12 LAST WRITINGS ON THE PHILOSOPHY OF PSYCHOLOGY 2\n",
      "Wittgenstein - Notebooks 1914-1916 ed 1961\n",
      "03 PHILOSOPHICAL GRAMMAR\n",
      "13 ON CERTAINTY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pdf_file in Path('data/raw').glob('*.pdf'):\n",
    "    file_name = pdf_file.name.split('.')[0] \n",
    "    print(file_name)\n",
    "    # text = extract_pdf(pdf_file, file_name)\n",
    "    # save_file(text, file_name + '.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T20:11:38.647381Z",
     "start_time": "2024-02-03T20:10:39.959685Z"
    }
   },
   "id": "7db7668a0d9bda18",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read into df applying normalization:\n",
    "### Splitting into sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5aa5095c8b688b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if isinstance(text, str):\n",
    "        # remove multiple whitespaces\n",
    "        text = re.sub(r\"\\s+\" , \" \", text)\n",
    "        # remove same repeating characters\n",
    "        text = re.sub(r\"(\\w)\\1{2,}\", r\"\\1\", text)\n",
    "        # keep comments with at least 5 words\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text).lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T20:11:38.652302Z",
     "start_time": "2024-02-03T20:11:38.648443Z"
    }
   },
   "id": "fe57b90c7e083375",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['sentence', 'label_book'])\n",
    "pdf_map = {}\n",
    "# change to a different larger model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "(Path(\"data/wordcloud\").mkdir(parents=True, exist_ok=True))\n",
    "\n",
    "def generate_df(generate: bool = False):\n",
    "    if generate:\n",
    "        for index, pdf_file in enumerate(Path('data').glob('*.txt')):\n",
    "            file_name = pdf_file.name.split('.')[0]\n",
    "            pdf_map[index] = file_name\n",
    "            with open(pdf_file) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    sentences_preprocessed = []\n",
    "                    sentences_lemmatized = []\n",
    "                    # sentence tokenizer\n",
    "                    sentences = sent_tokenize(line)\n",
    "                    for sentence in sentences:\n",
    "                        sentence = preprocess(sentence) \n",
    "                        words = [word for word in sentence.split() if word.lower() not in stop_words and len(word) > 2]\n",
    "                        sentence_clean = ' '.join(words)\n",
    "                        # add only sentences with at least 3 words\n",
    "                        if len(sentence_clean.split(' ')) <= 2:\n",
    "                            continue\n",
    "                        # start preprocess\n",
    "                        sentences_preprocessed.append(sentence_clean)\n",
    "\n",
    "                    for doc in nlp.pipe(sentences_preprocessed, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"pos_tagger\", \"ner\"]):\n",
    "                        # Do something with the doc here\n",
    "                        sentence_lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "                        sentences_lemmatized.append(sentence_lemmatized)\n",
    "                    # wordclouds save\n",
    "                    try:\n",
    "                        word_cloud = WordCloud(\n",
    "                                collocations=False,\n",
    "                                background_color='black',\n",
    "                                width=2048, height=1080,\n",
    "                                ).generate(\" \".join(sentences_lemmatized))\n",
    "                        word_cloud.to_file(Path(\"data/wordcloud\") / f\"{file_name}_wc.png\")\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    labels = np.full(len(sentences_lemmatized), file_name)\n",
    "                    temp_df = pd.DataFrame({'sentence': sentences_lemmatized, 'label_book': index})\n",
    "                    df = pd.concat([df, temp_df])\n",
    "                    df.to_csv((Path(\"data\")) / \"dataframe.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv((Path(\"data\")) / \"dataframe.csv\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T08:49:00.234669Z",
     "start_time": "2024-02-04T08:48:58.859644Z"
    }
   },
   "id": "d2bc938025c2770b",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T06:44:42.939357Z",
     "start_time": "2024-02-04T06:44:42.872993Z"
    }
   },
   "id": "1a455c723bb64a6",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0                                           sentence  \\\n0               0                        world totality facts things   \n1               1                       world determined facts facts   \n2               2  totality facts determines case lso casethe fac...   \n3               3                                world divides facts   \n4               4  ach item case case hing remains samewhat casea...   \n...           ...                                                ...   \n38307        1075                             melding bothas tempted   \n38308        1076                          asks way difficult answer   \n38309        1077  arrow end remark shows connected following remark   \n38310        1078                      sentence obviously incomplete   \n38311        1079  manuscript paragraph originally read partially...   \n\n       label_book  \n0               0  \n1               0  \n2               0  \n3               0  \n4               0  \n...           ...  \n38307          15  \n38308          15  \n38309          15  \n38310          15  \n38311          15  \n\n[38312 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>sentence</th>\n      <th>label_book</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>world totality facts things</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>world determined facts facts</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>totality facts determines case lso casethe fac...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>world divides facts</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>ach item case case hing remains samewhat casea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38307</th>\n      <td>1075</td>\n      <td>melding bothas tempted</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>38308</th>\n      <td>1076</td>\n      <td>asks way difficult answer</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>38309</th>\n      <td>1077</td>\n      <td>arrow end remark shows connected following remark</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>38310</th>\n      <td>1078</td>\n      <td>sentence obviously incomplete</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>38311</th>\n      <td>1079</td>\n      <td>manuscript paragraph originally read partially...</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>38312 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T08:49:02.541885Z",
     "start_time": "2024-02-04T08:49:02.532619Z"
    }
   },
   "id": "20dbf93763dcef7f",
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
